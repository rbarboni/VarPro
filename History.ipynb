{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ddc9ef",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbfc4bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "def add_one_row(x):\n",
    "    return torch.cat((x, torch.ones(x.shape[0]).view((x.shape[0],1))), dim=1)\n",
    "\n",
    "def model_plot_2d(model, add_one=False, N_points=100, x_lim=(-5,5), plot=True): ## Data plot\n",
    "    x = np.linspace(*x_lim, N_points)\n",
    "    y = np.linspace(*x_lim, N_points)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    if add_one:\n",
    "        inputs = torch.tensor(np.stack((xx.flatten(), yy.flatten(), np.ones(N_points**2))), dtype=torch.float32).T\n",
    "    else:\n",
    "        inputs = torch.tensor(np.stack((xx.flatten(), yy.flatten())), dtype=torch.float32).T\n",
    "    zz = model(inputs).detach().view((N_points, N_points)).numpy()\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "        surf = ax.plot_surface(xx, yy, zz, linewidth=0, antialiased=False, cmap=cm.coolwarm)\n",
    "        fig.colorbar(surf)\n",
    "        plt.show()\n",
    "    return xx, yy, zz\n",
    "\n",
    "def model_plot_1d(model, add_one=False, N_points=1000, x_lim=(-10,10)): ## Data plot\n",
    "    x = np.linspace(*x_lim, N_points)\n",
    "    if add_one:\n",
    "        inputs = torch.tensor(np.stack((x, np.ones(N_points))), dtype=torch.float32).T\n",
    "    else:\n",
    "        inputs = torch.tensor(x, dtype=torch.float32).view((N_points, -1))\n",
    "    y = model(inputs).detach().squeeze().numpy()\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "\n",
    "def weight_animation_2d(weight_array, name='animation.mp4'):\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    x_min, x_max = np.min(weight_array[:,:,0])-0.2, np.max(weight_array[:,:,0])+0.2\n",
    "    y_min, y_max = np.min(weight_array[:,:,1])-0.2, np.max(weight_array[:,:,1])+0.2\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    points, = ax.plot([], [], 'o', c='blue', ms=3)\n",
    "    def animate(i):\n",
    "        points.set_data(weight_array[i,:,0], weight_array[i,:,1])\n",
    "        return points\n",
    "    \n",
    "    time = 20\n",
    "    fps = 10\n",
    "    frames = time * fps\n",
    "    rate = weight_array.shape[0] // frames\n",
    "    anim = animation.FuncAnimation(fig, lambda i: animate(rate*i), frames, blit=False, interval=1)\n",
    "    writer = animation.FFMpegWriter(fps=fps)\n",
    "    anim.save(name, writer = writer)\n",
    "\n",
    "\n",
    "def circle_to_line(x):\n",
    "    return 2 * np.arctan( x[:,1] / (1+x[:,0]))\n",
    "\n",
    "## density of diracs (with position in x on given interval) convolved with gaussians\n",
    "def gaussian_conv(x, coef=None, scale=1, interval=(-np.pi, np.pi), N_points=1000):\n",
    "    res = np.zeros(N_points)\n",
    "    z_min, z_max = interval\n",
    "    z = np.linspace(z_min, z_max, N_points+1)\n",
    "    z = 0.5*(z[1:]+z[:-1])\n",
    "    if coef is None:\n",
    "        coef = np.ones(len(x)) / len(x)\n",
    "    for i in range(len(x)):\n",
    "        res += coef[i] * np.exp(- 0.5 * (z_min + (z-x[i]-z_min) % (z_max-z_min))**2 / scale**2)\n",
    "    return z, normalize(res, interval=interval)\n",
    "\n",
    "def Laplacian(f, h=1):\n",
    "    return (np.roll(f, 1) + np.roll(f, -1) - 2*f) / h**2\n",
    "\n",
    "def Grad(f, h=1):\n",
    "    return (f - np.roll(f, 1)) / h\n",
    "\n",
    "def normalize(f, interval=(-np.pi,np.pi)):\n",
    "    z_min, z_max = interval\n",
    "    return len(f) * f / (f.sum() * (z_max-z_min))\n",
    "\n",
    "def pmax(x, t):\n",
    "    return torch.maximum(x, t*torch.ones_like(x))\n",
    "\n",
    "def pmin(x, t):\n",
    "    return torch.minimum(x, t*torch.ones_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a764a1b",
   "metadata": {},
   "source": [
    "# Training Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985e868",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, optimizer, criterion, noisy=False, tau=0.1):\n",
    "    loss_list = []\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(inputs, targets, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        if noisy:\n",
    "            with torch.no_grad():\n",
    "                for group in optimizer.param_groups:\n",
    "                    for p in group[\"params\"]:\n",
    "                        p.add_(tau * torch.randn_like(p))\n",
    "        if hasattr(model, 'clipper'):\n",
    "            model.clipper(model)\n",
    "    return loss_list\n",
    "\n",
    "class LearningProblem():\n",
    "    def __init__(self, model, train_loader, optimizer, criterion):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.loss_list = []\n",
    "        self.state_list = [copy.deepcopy(self.model.state_dict())]\n",
    "\n",
    "    def train(self, epochs, noisy=False, tau=0.1):\n",
    "        self.model.train()\n",
    "        iterator = tqdm(range(epochs))\n",
    "        for i in iterator:\n",
    "            loss = training_loop(self.model, self.train_loader, self.optimizer, self.criterion, noisy=noisy, tau=tau)\n",
    "            self.loss_list.extend(loss)\n",
    "            self.state_list.append(copy.deepcopy(self.model.state_dict()))\n",
    "            iterator.set_description(f'log10(loss) = {np.log10(self.loss_list[-1]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03d0e9",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ed062",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationFunction(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "class VarProModel(nn.Module):\n",
    "    def __init__(self, feature_model, width, output_dim, VarProTraining=True, clipper=None):\n",
    "        super().__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.VarProTraining = VarProTraining\n",
    "        self.width = width\n",
    "        self.outer = nn.Linear(width, output_dim, bias=False)\n",
    "        if VarProTraining:\n",
    "            self.outer.weight.requires_grad = False\n",
    "        if clipper is not None:\n",
    "            self.clipper = clipper\n",
    "            self.clipper(self)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.outer(self.feature_model(inputs)) / self.width ## normalized output function\n",
    "\n",
    "class SHLFeatureModel(nn.Module):\n",
    "    def __init__(self, input_dim, width, activation, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(data=torch.randn(width, input_dim), requires_grad=True)\n",
    "        self.activation = activation\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(data=torch.zeros(width), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(nn.functional.linear(x, self.weight, self.bias))\n",
    "\n",
    "class SignedSHLFeatureModel(nn.Module):\n",
    "    def __init__(self, input_dim, width, activation, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(data=torch.randn(width, input_dim), requires_grad=True)\n",
    "        self.sign = nn.Parameter(data=torch.randn(width), requires_grad=True)\n",
    "        self.activation = activation\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(data=torch.zeros(width), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(nn.functional.linear(x, self.weight, self.bias)) @ torch.diag(self.sign)\n",
    "\n",
    "class ConvolutionFeatureModel(nn.Module):\n",
    "    def __init__(self, input_dim, width, activation, scale):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(data=torch.randn(width, input_dim), requires_grad=True)\n",
    "        self.activation = activation\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(torch.linalg.vector_norm(self.weight.T[None,:,:] - x[:,:,None], dim=1) / self.scale)\n",
    "\n",
    "def SHL(input_dim, width, activation, bias=False, VarProTraining=True, clipper=None):\n",
    "    feature_model = SHLFeatureModel(input_dim, width, activation, bias=bias)\n",
    "    return VarProModel(feature_model, width, 1, VarProTraining=VarProTraining, clipper=clipper)\n",
    "\n",
    "def SignedSHL(input_dim, width, activation, bias=False, VarProTraining=True, clipper=None):\n",
    "    feature_model = SignedSHLFeatureModel(input_dim, width, activation, bias=bias)\n",
    "    return VarProModel(feature_model, width, 1, VarProTraining=VarProTraining, clipper=clipper)\n",
    "\n",
    "def Convolution(input_dim, width, activation, scale, VarProTraining=True, clipper=None):\n",
    "    feature_model = ConvolutionFeatureModel(input_dim, width, activation, scale)\n",
    "    return VarProModel(feature_model, width, 1, VarProTraining=VarProTraining, clipper=clipper)\n",
    "\n",
    "def freeze(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016b7ae",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce37e20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## VarPro for quadratic loss\n",
    "class ExactRidgeProjection():\n",
    "    def __init__(self, lmbda, keep_alpha=False):\n",
    "        self.lmbda = lmbda\n",
    "        self.keep_alpha = keep_alpha\n",
    "        if self.keep_alpha:\n",
    "            self.alpha_list = []\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, inputs, targets, model, requires_grad=False):\n",
    "        features = model.feature_model(inputs).clone().detach()\n",
    "        batch_size, width = features.shape[0], features.shape[1]\n",
    "        if not self.keep_alpha and batch_size > width: ## underparameterized case\n",
    "            K = (features.T @ features) / (batch_size * width)\n",
    "            u = torch.linalg.solve(K + self.lmbda * torch.eye(width),  (features.T @ targets) / batch_size)\n",
    "        else: ## overparameterized case\n",
    "            K = (features @ features.T) / (batch_size*width)\n",
    "            alpha = torch.linalg.solve(K + self.lmbda * torch.eye(batch_size), targets)\n",
    "            u = features.T @ alpha / batch_size\n",
    "            if self.keep_alpha:\n",
    "                self.alpha_list.append(alpha)\n",
    "        model.outer.weight = nn.Parameter(data=u.view((1,-1)), requires_grad=requires_grad)\n",
    "\n",
    "class ExactRidgeProjectionUnbiased():\n",
    "    def __init__(self, lmbda):\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, inputs, targets, model, requires_grad=False):\n",
    "        features = model.feature_model(inputs).clone().detach()\n",
    "        batch_size, width = features.shape[0], features.shape[1]\n",
    "        K = (features.T @ features) / (batch_size * width)\n",
    "        u = torch.linalg.solve(K + self.lmbda * torch.eye(width),  (features.T @ targets) / batch_size + self.lmbda)\n",
    "        model.outer.weight = nn.Parameter(data=u.view((1,-1)), requires_grad=requires_grad)\n",
    "\n",
    "class ApproximateRidgeProjection():\n",
    "    def __init__(self, lmbda, method='cg', tol=1e-7):\n",
    "        self.lmbda = lmbda\n",
    "        self.method = method\n",
    "        self.tol = tol\n",
    "\n",
    "    def __call__(self, inputs, targets, model):\n",
    "        features = model.feature_model(inputs).clone().detach()\n",
    "        width = features.shape[1]\n",
    "        ridge_objective = lambda u: ((features @ u / width - targets)**2).mean() + self.lmbda * (u**2).mean()\n",
    "        u = minimize(ridge_objective, model.outer.weight.detach().squeeze(), method=self.method, tol=self.tol).x\n",
    "        model.outer.weight = nn.Parameter(data=u.view((1,-1)), requires_grad=False)\n",
    "\n",
    "class LeastSquareCriterion(nn.Module):\n",
    "    def __init__(self, lmbda, projection=None):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "        self.projection = projection\n",
    "        \n",
    "    def forward(self, inputs, targets, model):\n",
    "        if self.projection is not None:\n",
    "            self.projection(inputs, targets, model)\n",
    "        predictions = model(inputs)\n",
    "        return 0.5 * ((predictions - targets)**2).mean() / self.lmbda  + 0.5 * (model.outer.weight**2).mean()\n",
    "\n",
    "class LeastSquareCriterionUnbiased(nn.Module):\n",
    "    def __init__(self, lmbda, projection=None):\n",
    "        super().__init__()\n",
    "        self.lmbda = lmbda\n",
    "        self.projection = projection\n",
    "        \n",
    "    def forward(self, inputs, targets, model):\n",
    "        if self.projection is not None:\n",
    "            self.projection(inputs, targets, model)\n",
    "        predictions = model(inputs)\n",
    "        return 0.5 * ((predictions - targets)**2).mean() / self.lmbda  + 0.5 * ((model.outer.weight-1)**2).mean()\n",
    "\n",
    "class Normalization():\n",
    "    def __call__(self, w):\n",
    "        return w / torch.norm(w, 2, dim=-1, keepdim=True).expand_as(w)    \n",
    "\n",
    "class PeriodicBoundaryCondition():\n",
    "    def __init__(self, x_min=-1, x_max=1):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.x_min + (w-self.x_min) % (self.x_max-self.x_min)\n",
    "\n",
    "class BallClipper():\n",
    "    def __init__(self, radius=1):\n",
    "        self.radius = radius\n",
    "\n",
    "    def __call__(self, w):\n",
    "        norm = w.norm(dim=1, keepdim=True).expand_as(w)\n",
    "        return (w / norm) * pmin(norm, self.radius)\n",
    "\n",
    "# thresholding weights value\n",
    "class Thresholding():\n",
    "    def __init__(self, x_min=-1, x_max=1):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return pmax(pmin(w, self.x_max), self.x_min)\n",
    "        \n",
    "# Apply clipping function on feature model weights\n",
    "class FeatureClipper():\n",
    "    def __init__(self, clipping_function):\n",
    "        self.clipping_function = clipping_function\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, model):\n",
    "        w = model.state_dict()['feature_model.weight']\n",
    "        w.copy_(self.clipping_function(w))\n",
    "\n",
    "# Apply clipping function on feature model weights and biases (only for SHL)\n",
    "class FeatureBiasClipper():\n",
    "    def __init__(self, weight_clipper, bias_clipper):\n",
    "        self.weight_clipper = weight_clipper\n",
    "        self.bias_clipper = bias_clipper\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, model):\n",
    "        dico = model.state_dict()\n",
    "        w, b = dico['feature_model.weight'], dico['feature_model.bias']\n",
    "        w.copy_(self.weight_clipper(w))\n",
    "        b.copy_(self.bias_clipper(b))\n",
    "\n",
    "# Apply clipping function on feature model weights and signs (only for SignedSHL)\n",
    "class FeatureSignClipper():\n",
    "    def __init__(self, weight_clipper, sign_clipper):\n",
    "        self.weight_clipper = weight_clipper\n",
    "        self.sign_clipper = sign_clipper\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, model):\n",
    "        dico = model.state_dict()\n",
    "        w, s = dico['feature_model.weight'], dico['feature_model.sign']\n",
    "        w.copy_(self.weight_clipper(w))\n",
    "        s.copy_(self.sign_clipper(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc0483",
   "metadata": {},
   "source": [
    "# Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c45bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def relu_kernel_matrix(w1, w2):\n",
    "    Theta = torch.acos(pmax(pmin(w1 @ w2.T, 1), -1))\n",
    "    return (torch.sin(Theta) + (np.pi-Theta) * torch.cos(Theta)) / (2*np.pi)\n",
    "\n",
    "class GaussianKernel():\n",
    "    def __init__(self, gamma=1):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return torch.exp(-x.norm(dim=-1)**2 / self.gamma)\n",
    "\n",
    "class EnergyKernel():\n",
    "    def __call__(self, x):\n",
    "        return -x.norm(dim=-1)\n",
    "\n",
    "class DistanceMMD():\n",
    "    def __init__(self, kernel=EnergyKernel(), projection=nn.Identity()):\n",
    "        self.kernel = kernel\n",
    "        self.projection = projection\n",
    "\n",
    "    def __call__(self, m1, m2, c1, c2):\n",
    "        K1 = self.kernel(self.projection(m1[:,None,:] - m1[None,:,:]))\n",
    "        K2 = self.kernel(self.projection(m2[:,None,:] - m2[None,:,:]))\n",
    "        K3 = self.kernel(self.projection(m1[:,None,:] - m2[None,:,:]))\n",
    "        return (c1.dot(K1@c1) + c2.dot(K2@c2) - 2*c1.dot(K3@c2)).sqrt()\n",
    "\n",
    "class DistanceOT():\n",
    "    def __init__(self, projection=nn.Identity()):\n",
    "        self.projection = projection\n",
    "\n",
    "    def __call__(self, m1, m2, c1, c2):\n",
    "        M = (self.projection(m1[:,None,:] - m2[None,:,:])**2).sum(dim=-1).numpy()\n",
    "        return np.sqrt(ot.emd2(c1.numpy(), c2.numpy(), M))\n",
    "\n",
    "def compute_distance(distance, weight_list, weight_ref, c_list=None, c_ref=None, N_eval=100):\n",
    "\n",
    "    if c_list is None:\n",
    "        c = torch.ones(weight_list[0].shape[0]) / weight_list[0].shape[0]\n",
    "        c_list = [c for _ in range(len(weight_list))]\n",
    "    if c_ref is None:\n",
    "        c_ref = torch.ones(weight_ref.shape[0]) / weight_ref.shape[0]\n",
    "\n",
    "    distance_list = []\n",
    "    idx = np.array([int(i) for i in np.linspace(0, len(weight_list)-1, N_eval+1)])\n",
    "    for i in tqdm(idx):\n",
    "        distance_list.append(distance(weight_ref, weight_list[i], c_ref, c_list[i]).item())\n",
    "    return distance_list, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc9b21",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c21645",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TeacherCosFunction():\n",
    "    def __call__(self, inputs):\n",
    "        norm = inputs.norm(dim=1)\n",
    "        cos = inputs[:,0] / norm\n",
    "        return 0.5 * norm * cos\n",
    "\n",
    "class TeacherGammaFunction():\n",
    "    def __init__(self, gamma):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        rotated_inputs = inputs @ torch.tensor([[1, 0], [0, (1+self.gamma)**0.5]])\n",
    "        norm = rotated_inputs.norm(dim=1)\n",
    "        cos = rotated_inputs[:,0] / norm\n",
    "        sin = rotated_inputs[:,1] / norm\n",
    "        r1 = cos * torch.asinh(cos*gamma**0.5)\n",
    "        r2 = sin * torch.asin(sin*(gamma/(1+gamma))**0.5)\n",
    "        return norm * (r1+r2) / (np.pi*gamma**0.5)\n",
    "\n",
    "class TeacherStepFunction():\n",
    "    def __init__(self, alpha, eps):\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.beta = (1-2*(np.pi-eps)*alpha) / (2*eps)\n",
    "        #assert self.beta > 0\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        inputs_array = inputs.clone().detach().numpy()\n",
    "        norm = np.linalg.norm(inputs_array, axis=1)\n",
    "        cos = inputs_array[:,0] / norm\n",
    "        sin = inputs_array[:,1] / norm\n",
    "        theta = 2 * np.arctan(sin / (1+cos))\n",
    "        \n",
    "        res = ((theta > -0.5*np.pi-self.eps)*(theta < -0.5*np.pi+self.eps)) * (1+np.sin(theta+self.eps))\n",
    "        res += ((theta > 0.5*np.pi-self.eps)*(theta < 0.5*np.pi+self.eps)) * (1-np.sin(theta-self.eps))\n",
    "        res += ((theta > -0.5*np.pi+self.eps)*(theta < 0.5*np.pi-self.eps)) * (np.sin(theta+self.eps)-np.sin(theta-self.eps))\n",
    "    \n",
    "        return torch.tensor((2 * self.alpha + (self.beta-self.alpha) * res) * norm, dtype=torch.float32)\n",
    "\n",
    "## Data\n",
    "d = 1\n",
    "N = 2048\n",
    "gamma = 10\n",
    "\n",
    "alpha = 0.2 * (1 / 6.28)\n",
    "eps = 0.1\n",
    "\n",
    "teacher_function = TeacherGammaFunction(gamma)\n",
    "\n",
    "inputs = torch.randn(N,2)\n",
    "#rotated_inputs = inputs @ torch.tensor([[0, -1], [1, 0]], dtype=torch.float32)\n",
    "#targets = 0.5*(teacher_function(inputs) + teacher_function(rotated_inputs)).view((N, 1))\n",
    "targets = teacher_function(inputs).view((N, 1))\n",
    "dataset = CustomDataset(inputs, targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91f071",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TeacherStudentProblem(LearningProblem):\n",
    "    def __init__(self, student, teacher, inputs, optimizer, criterion):\n",
    "        self.teacher = teacher\n",
    "        self.dataset = CustomDataset(inputs, self.teacher(inputs).clone().detach())\n",
    "        train_loader = torch.utils.data.DataLoader(self.dataset, batch_size=len(self.dataset))\n",
    "        super().__init__(student, train_loader, optimizer, criterion)\n",
    "        \n",
    "        self.distance_dict = {}\n",
    "        \n",
    "    def compute_distance(self, distance, weight_ref=None, c_ref=None, name='teacher', distance_name=None, N_eval=100):\n",
    "        \n",
    "        weight_list = [dico['feature_model.0.weight'] for dico in self.state_list]\n",
    "        \n",
    "        if weight_ref is None:\n",
    "            name = 'teacher'\n",
    "            weight_ref = self.teacher.feature_model[0].weight.clone().detach()\n",
    "            c_ref = self.teacher.outer.weight.detach().squeeze()\n",
    "            c_ref.div_(c_ref.norm(p=1))\n",
    "            \n",
    "        distance_list, idx = compute_distance(distance, weight_list, weight_ref, c_ref=c_ref, N_eval=N_eval)\n",
    "        step = len(self.loss_list) // (len(self.state_list)-1)\n",
    "\n",
    "        self.distance_dict[name] = {'distance_name': distance_name, 'distance_list': distance_list, 'idx': step*idx}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
